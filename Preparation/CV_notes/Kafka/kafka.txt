
1. Kafka batches and partitions. 

For efficiency, messages are written into Kafka in batches. 
A batch is just a collection of messages, 
all of which are being produced to the same topic and partition. 
An individual roundtrip across the network for each message 
would result in excessive over head, 
and collecting messages together into a batch reduces this. 
Of course, this is a tradeoff between latency and throughput

Partitions are also the way that Kafka provides redundancy and scalability. 
Each partition can be hosted on a different server, 
which means that a single topic can be scaled horizontally across multiple servers 
to provide performance far beyond the ability of a single server

The offset is another bit of metadata
—an integer value that continually increases
—that Kafka adds to each message as it is produced. 
Each message in a given partition has a unique offset. 
By storing the offset of the last consumed message for each partition, 
either in Zookeeper or in Kafka itself, 
a consumer can stop and restart without losing its place

Consumers work as part of a consumer group, 
which is one or more consumers that work together to consume a topic.
The group assures that each partition is only consumed by one member.
The mapping of a consumer to a partition is often called 
ownership of the partition by the consumer.

In this way, consumers can horizontally scale to consume topics 
with a large number of messages. 
Additionally, if a single consumer fails, 
the remaining members of the group will rebalance the partitions 
being consumed to take over for the missing member. 

2. Brokers and Clusters

A single Kafka server is called a broker. 
The broker receives messages from producers, assigns offsets to them, 
and commits the messages to storage on disk.

A partition may be assigned to multiple brokers, 
which will result in the partition being replicated (as seen in Figure 1-7). 
This provides redundancy of messages in the partition, 
such that another broker can take over leadership if there is a broker failure. 
However, all consumers and producers operating 
on that partition must connect to the leader.

3. Cross region replication. 

The Kafka project includes a tool called MirrorMaker, used for this purpose. 
At its core, MirrorMaker is simply a Kafka consumer and producer, 
linked together with a queue. 
Messages are consumed from one Kafka cluster and produced for another.

4. Benefits.

Multiple Producers

This makes the system ideal for aggregating data from
many frontend systems and making it consistent. 
For example, a site that serves content to users via a number of microservices 
can have a single topic for page views that
all services can write to using a common format

Multiple Consumers

Kafka is designed for multiple consumers to read
any single stream of messages without interfering with each other. 
This is in contrast to many queuing systems where 
once a message is consumed by one client, 
it is not available to any other.

When coupled with a system to provide message schemas, 
producers and consumers no longer require tight coupling 
or direct connections of any sort. 

Goals for Kafka: 

 - Decouple producers and consumers by using a push-pull model
 - Provide persistence for message data within the messaging system to allow multiple consumers
 - Optimize for high throughput of messages
 - Allow for horizontal scaling of the system to grow as the data streams grew

Many users will have the partition count for a topic be equal to, 
or a multiple of, the number of brokers in the cluster.
This allows the partitions to be evenly distributed to the brokers, 
which will evenly distribute the message load.

5. Producers. 
can we tolerate loss of messages? 
Are we OK with accidentally duplicating messages?
Are there any strict latency or throughput requirements we need to support?

we can see that it is critical to never lose a single message nor duplicate any messages. 
Latency should be low but latencies up to 500ms can be tolerated, 
and throughput should be very high—
we expect to process up to a million messages a second.

 - Send Record to partitiner 
 - Calculate partition based on Key 
 - add message to a batch of records for partion
 - separate thread sends batches to brokers 
 - if fails producer may retry this message 
 
Ways to send messages: 
 1. Fire-and-forget
 2. Synchronous send
 3. Asynchronous send - send 100 messages and results will be received in callback.
 
While all the examples in this chapter are single threaded, 
a producer object can be used by multiple threads to send messages. 

You will probably want to start with one producer and one thread. 
If you need better throughput, you can add more threads that use the same producer. 
Once this ceases to increase throughput, 
you can add more producers to the application to achieve even higher throughput.

acks - The acks parameter controls how many partition replicas 
must receive the record before the producer can consider the write successful.

  acks=0  
    no ack from partition. Message can be lost. High throughput
	
  acks=1
    the producer will receive a success response from the broker 
	the moment the leader replica received the message. 
	If the message can’t be written to the leader 
	(e.g., if the leader crashed and a new leader was not elected yet), 
	the producer will receive an error response and can retry sending the message, avoiding potential loss of data. 
	
	The message can still get lost if the leader crashes and a replica without this message 
	gets elected as the new leader (via unclean leader election). 
	
	In this case, throughput depends on whether we send messages synchronously or asynchronously. 
	If our client code waits for a reply from the server 
	(by calling the get() method of the Future object returned when sending a message)
	it will obviously increase latency significantly (at least by a network roundtrip). 
	
	If the client uses callbacks, latency will be hidden, 
	but throughput will be limited by the number of in-flight messages 
	(i.e., how many messages the pro-ducer will send before receiving replies from the server)
	
  acks=all 
    the producer will receive a success response from the broker once all in-sync replicas received the message. 
	This is the safest mode since you can make sure more than one broker 
	has the message and that the message will survive even in the case of crash	
	
  batch.size
    When multiple records are sent to the same partition, 
	the producer will batch them together. 
	This parameter controls the amount of memory in bytes (not messages!) that will be used for each batch.  

6. Partitions.
  All messages with the same key will go to the same partition. 
  This means that if a process is reading only a subset of the partitions in a topic, 
  all the records for a single key will be read by the same process.
  
  Custom partition strategy.
  
  Suppose that you do so much business with customer “Banana” 
  that over 10% of your daily transactions are with this customer. 
  If you use default hash partitioning, the Banana records will get allocated to the same
  partition as other accounts, resulting in one partition being about 
  twice as large as the rest. 
  
  This can cause servers to run out of space, processing to slow down, etc. 
  What we really want is to give Banana its own partition 
  and then use hash partitioning to map the rest of the accounts to partitions.
  
  
  
  



	
	
	


 
































 