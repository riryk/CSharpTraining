
https://pages.awscloud.com/acq_NAMER_Partner-GSC-Serverless-eBook-Download-Page.html
https://www.redhat.com/architect/pros-and-cons-strangler-architecture-pattern#:~:text=The%20Strangler%20pattern%20is%20one,system%20pass%20through%20the%20facade.


It promises developers the ability to iterate as
fast as possible while maintaining business critical latency, availability, security, and
performance guarantees, with minimal effort on the developers’ part.

The “software as a service” consumption model is
well understood. It means that anyone using the software uses a prescribed
application programming interface (API) or web interface to use the software
and customize it

Incurs cost only when used—This means there’s zero cost for having the software
deployed and ready to use. 

We define microservices as small, standalone, fully independent services 
built around a particular business purpose or capability. 
Ideally, microservices should be easy to replace, 
with each service written in an appropriate framework and language.

Each microservice can maintain state and store data. 
And if microservices are correctly decoupled, 
development teams can work and deploy microservices independently from one another. 
This approach of building and deploying applications as a collection of loosely coupled services 
is considered the default approach to development in the cloud today.

 1. Developing your desired services traditionally requires servers running in data centers 
    or in the cloud that need to be managed, maintained, patched, and backed up.
 2. Directly build on VMs - 
    Provisioning, managing, and patching of these servers is a time-consuming task 
	that often requires dedicated operations people.
 3. Infrastructure as a service. 
    Rent VM in the Cloud. It will be managed by AWS team. Patches, maintenance, 
	back-ups. 
 4. Platform as a service
    and containers have appeared as potential solutions to the headache of 
	inconsistent infrastructure environments, conflicts, and server management overhead. 
	PaaS is a form of cloud computing that provides a platform for users
    to run their software while hiding some of the underlying infrastructure. 
 5. Use containers
    Containerization is considered ideal for microservices architectures
	because it is a way of isolating an application with its own environment.
    It’s a lightweight alternative to full-blown virtualization that traditional cloud servers use. 

Drawbacks of all these approaches: 

While each of these models are perfectly valid and offer varying degrees 
of simplicity and speed of development for your service, 
your costs are still driven by the lifecycle of the infrastructure or servers you own, 
not to your application usage.

If you purchase a rack at the data center, you pay for it 24/7. 
If you purchase a cloud instance 
(wrapped in a PaaS or running containers or otherwise), 
you pay for it when it runs, independent of whether it is serving traffic for your web app or not. 

Serverless - pay only when you use it. 

For example, to build the API, we would use a service that does not cost us anything if there are no API calls.
Serverless architecture uses existing services from cloud providers 
like AWS to implement its architectural components

  AWS offers services to build our application primitives like APIs (Amazon API Gateway), 
  workflows (AWS Step Functions), 
  queues (Amazon Simple Queue Service), 
  databases (Amazon DynamoDB and Amazon Aurora)
 
Function as a service - 
  This ability to run arbitrary code without having to provision infrastructure 
  to run it as a service or to pay for the infrastructure
  is referred to as functions as a service 
  
You pay only for the time your code runs. 
A function is not a lightweight instance; 
instead, think of it as akin to processes in an OS, 
where you can spawn as many as needed by your application 
and then spin them down when your application isn’t running.

A function is not a lightweight instance; 
instead, think of it as akin to processes in an OS, 
where you can spawn as many as needed by your application 
and then spin them down when your application isn’t running. 

By making use of various powerful single-purpose APIs and web services, 
developers can build loosely coupled, scalable, and efficient architectures quickly. 

He once tweeted, “serverless != functions, 
FaaS == functions, serverless == on-demand scaling 
and pricing characteristics (not limited to functions)

An emerging trend is that of serverless containers; 
that is, leveraging containers instead of functions 
to implement the custom logic and using the container as a utility service 
and incurring costs only when the container runs. 

Services like AWS Fargate or Google Cloud Run offer this capability. 
The difference between the two (functions vs. containers) 
is just the degree to which developers want 
to shift the boundaries of shared responsibilities.

 Serverless technologies feature automatic scaling, built-in high availability, 
 and a pay-for-use billing model to increase agility and optimize costs.
 
  - Eliminate operational overhead
  - Lower your costs. 
      With a pay-for-value billing model, resource utilization is automatically optimized 
	  and you never pay for over-provisioning
  - Adapt at scale
  - Serverless applications have built-in service integrations, 
    so you can focus on building your application instead of configuring it.  

AWS Lambda
   Compute service that runs your code in response to events and automatically manages the compute resources.

AWS Step Functions
   Coordinate components of distributed applications.

Amazon SQS
   Scalable queue for storing messages as they travel between computers.

Amazon SNS
   Fast, flexible, fully managed push messaging service.

Amazon API Gateway
   Publish, maintain, monitor, and secure APIs at any scale.

Lambda functions are stateless and scalable, 
which makes them perfect for implementing any logic that benefits from parallel processing. 
The most flexible and powerful serverless designs are event-driven, 
which means each component in the architecture reacts to a state change 
or notification of some kind rather than responding to a request or polling for information

Recommendations for AWS Serverless: 

1. Avoid lift-and-shift—In practice, serverless architectures are more suited for new
applications rather than porting existing applications over. 
This is because existing application code bases have a lot of code
that is made redundant by the serverless services.

2. Adopt a serverless first approach, not a serverless only approach
 build as much as possible using third-party services, 
 fall back to custom services built using AWS serverless primitives like AWS Lambda, 
 and finally, fall back to custom services built 
 using custom software running on infrastructure like EC2.
 
3. the strangler pattern 

4. Pick applications suited for a service-oriented architecture
Keep in mind that the serverless approach is all about reducing 
the amount of code you have to own and maintain, 
so you can iterate and innovate faster.
in-process calls and the need to handle failures and latency across a network
In more complex cases, developers can set up more elaborate pipelines
and orchestrate invocations of multiple functions.

5. Serverless pros and cons

Pros:
  High scale and reliability without server management
  Competitive pricing
  Less code
  
Cons:   
  Your application and business needs more control or you need to customize the infrastructure
  Your application and business needs require you to stay vendor agnostic
  
---------------------------------------------------------------------------
6. Use Cases.

A serverless application backend is appealing because it removes a lot of infrastructure management, 
has granular and predictable billing 
(especially when a serverless compute service such as Lambda is used), 
and can scale well to meet uneven demands.

1. Lambda and AWS services are well suited for building event-driven pipelines for data-processing tasks.
2. Real-time analytics
     Kinesis Data Streams and Lambda functions are a good fit for applications 
     that generate a lot of data that needs to be analyzed, aggregated, and stored.
3. Legacy API proxy
4. Scheduled service
5. Hybrids
6. Command pattern
     ApiGateway -> CommandLambda -> calls to other Lambdas -> call to db, file system
7. Messaging pattern
     The reliability comes from the fact that if the consuming service goes offline, 
	 the queue retains messages (for some period), which can still be processed at a later time.
	 
     Datasource -> Kinesis/SQS -> Lambda function/dispatcher -> call other Lambdas -> SQS -> Lambda dispatcher -> ...
	 
	 If you need to have multiple consumers, a straightforward way to do it is 
	 to introduce multiple queues into the system.
	 SQS queues can subscribe to an SNS topic so that 
	 pushing a message to the topic would automatically push 
	 the message to all of the subscribed queues.
	 
8. Priority queue pattern
     A great benefit of using a platform such as AWS and serverless architectures 
	 is that capacity planning and scalability 
	 are more of a concern for Amazon’s engineers than for you
	 
9. Fan-out pattern
     Fan-out pattern pushes a message to all listening/subscribed clients 
	 of a particular queue or a message pipeline. 
	 In AWS, this pattern is usually implemented using SNS topics 
	 that allow multiple subscribers to be invoked when a new message is added to a topic. 
	 
	 Take S3 as an example. When a new file is added to a bucket, 
	 S3 can invoke a single Lambda function with information about the file. 
	 
	 But what if you need to invoke two, three, or more Lambda functions at the same time? 
	 The original function could be modified to invoke other functions (like the command pattern), 
	 but that’s a lot of work if all you need is to run functions in parallel. 
	 The solution is to use the fan-out pattern with SNS
	 
	 This pattern is useful if you need to invoke multiple Lambda functions at the same time. 
	 An SNS topic will retry, invoking your Lambda functions, 
	 if it fails to deliver the message or if the function fails to execute
      
     SNS topics support other subscribers such as email and SQS queues. 
	 Adding a new message to a topic can invoke Lambda functions, 
	 send an email, or push a message on to an SQS queue, all at the same time. 
	 
10. Compute as glue
11. Pipes and filters pattern

------------------------------------------------------------------------
Yubl social network 
-----------------------------------------
1. Previous architecture 
 - A monolithic REST API written in Node.js and running on EC2
 - A WebSockets API written in Node.js and running on EC2
 - A monolithic MongoDB database hosted in MongoLab
 - A CloudAMQP message queue
 - A cluster of background workers written in Node.js and running on EC2 

Issues: 

 Scalability problems:
   These traffic spikes were usually short-lived, 
   which was problematic for the EC2-based system because EC2 autoscaling couldn’t react fast enough. 
   It typically takes EC2 instances a few minutes to spin up. 
   By the time they are ready to serve user requests, it’s too late.
   
   As a workaround, we ran a much larger EC2 cluster, 
   scaling up much earlier than we wanted. 
   This resulted in a lot of wasted cost because 
   we had to pay for lots of EC2 resources that we were not using. 
   Our cluster of API web servers had an average utilization of from 2% to 5%.
   
 Performance problems:
   One example of this is a user search, which is a frequently used API call 
   and executes a complex regex query against MongoDB. 
   Another example included user recommendations, which executed a complex query to find second
   - and third-degree connections to the current user
     those who follow your followers or those followed by users you follow
   	 
 Long feature delivery cycles:
   To make matters worse, every deployment required taking the whole system down for 30 minutes or more, 
   during which time users received no feedback and the app just appeared broken. 
   Features used to take months to go to production. 
   Even simple changes often took weeks to complete, 
   which was frustrating to everyone involved.
   
Why serverless?
  
  1. AWS Lambda autoscales the number of concurrent executions based on load. 
     This happens instantly and handles those unpredictable spikes we experience effortlessly.
	 
  2. AWS Lambda deploys functions to three availability zones by default, 
     which provides significant redundancy without incurring extra costs. 
	 We pay only when a function runs, whereas with EC2, 
	 we paid for the redundancy in a multi-AZ setup, which also dilutes the traffic 
	 and reduces the resource utilization even further.
  
  3. AWS manages the underlying physical infrastructure as well as the operating system 
     that our code runs on. 
	 AWS applies patches and security updates regularly 
	 and does a much better job of keeping the operating system secure than we could. 
	 This removes a whole class of vulnerabilities that plague 
	 so many software systems around the world.
	 
  4. With tools such as the Serverless framework, 
     the deployment pipeline for our application is drastically simplified. 
	 A typical deployment takes less than a minute 
	 and has no downtime because AWS Lambda automatically routes requests to the new code.

  5. When using serverless technologies such as API Gateway, Lambda, and DynamoDB, 
     we don’t have to worry about the underlying infrastructure. 
	 This lets us focus on addressing core business needs. 
	 Almost every line of our code is business logic! 
	 And it allows the development team to move quickly, 
	 knowing that what we build is scalable and resilient by default.
	 
  6. The number of production deployments went from four to six per month to averaging more
     than 80 per month with the same sized team. 
	 We didn’t have to hire more people to go faster, we allowed each developer to be more productive instead.

  7. As we migrated more and more of the system to serverless, scalability, 
     cost and reliability all improved. 
	 There were far fewer production issues, and we were spending a fraction of what we spent on EC2 previously.
	 
The new serverless Yubl architecture

Overall, the system’s reliability increased drastically. 
We experienced only one minor outage to our production environment 
because of a brief Simple Storage Service (S3) outage. 
The following points are some key highlights of the new serverless architecture on AWS:

-  The monolith was broken up into many microservices.
-  Every microservice has its own GitHub repository and one Continuous Integration/
   Continuous Delivery (CI/CD) pipeline. All the components that make up this
   microservice (API Gateway, Lambda functions, DynamoDB tables, etc.) are
   deployed together as one CloudFormation stack using the Serverless Framework.
-  Most microservices have an API Gateway REST API running under its own subdomain,
   such as search.yubl.com.
-  Every microservice has its own database for the data it needs. 
   Most use DynamoDB, but it’s not universal because different microservices have different data needs.
-  Every state change in the system is captured as an event and published to a Kinesis Data Stream 
   (for example, a user created new content, a user posted new content, and so on).
-  Most of the time, we prefer to synchronize data between microservices through
   events rather than synchronous API calls at run time. 
   This helps prevent cascade failures when one microservice experiences an outage in production.
   Instead, microservices subscribe to the relevant Kinesis Data Stream and copy needed data from the appropriate event
   
The goal was to deliver a better user experience with less downtime, 
more responsiveness, and more scalability. 

Rearchitecting and rewriting

The Strangler pattern is one in which an “old” system is put behind an intermediary facade. 
Then, over time external replacement services for the old system are added behind the facade.
The facade represents the functional entry points to the existing system. 
Calls to the old system pass through the facade. 

Behind the scenes, services within the old system are refactored into a new set of services. 
Once a new service is operational, the intermediary facade 
is modified to route calls that used to go to the service on the old system to the new service. 
Eventually, the services in the old system get "strangled" in favor of the new services.

Requires making sure that you have a rollback plan in play for each refactored instance. 
Things will go wrong. 
You need to be able to roll back to the old way of doing things quickly and safely.

Re-architecting user search feature: 
 In the Yubl app, you could search other users by first name, last name, and username. 
 This was a simple feature, but it caused crippling performance issues with the monolith as the number of users grew.
 This was because a search was implemented with regex queries against MongoDB. 
 The old implementation also didn’t allow for more sophisticated ranking, 
 so users often couldn’t find who they were looking for.
 
The new search API: 
 1. The legacy monolith publishes all user-related events to a Kinesis Data Stream called users. 
    These include the user-created and user-profile-updated events 
	that tell us when a new user joins or a user has updated their profile.
 2. A Lambda function subscribes to the users stream.
 3. The Lambda function uses these events to insert, update, or delete user documents 
    in the users index in Amazon CloudSearch.
 4. A new API in API Gateway with a POST /?query={string} endpoint proxies 
    to another Lambda function to handle the HTTP request.
 5. The Lambda function translates a user’s query string into 
    a search request against the users index in Amazon CloudSearch.
 6. To create a user-friendly subdomain for the new REST API, 
    a custom domain name in API Gateway for search.yubl.com is registered in Route53.
	
To do this, we ran a one-off task to copy all existing user data (~800,000 users) from MongoDB to CloudSearch, 
while tracking the most recent user profile update.	


 






   




 
	 


	 


  
	 


 
   
 
 
 
   
 

    
	




	 




 


  
  
 




   
	  
 
 






























 





