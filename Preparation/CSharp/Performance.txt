
1. Hewardt, Mario and Patrick Dussud. Advanced .NET Debugging. 
   Addison-Wesley  Professional, November 2009.
2  Richter, Jeffrey. CLR via C#, 4th ed. Microsoft Press, November 2012.
3  Russinovich, Mark and David Solomon, Alex Ionescu. Windows Internals, 6th ed. 
   Microsoft Press, March 2012.
4  Rasmussen, Brian. High-Performance Windows Store Apps. Microsoft Press, May 2014.
5  ECMA C# and CLI Standards: http://www.writinghighperf.net/go/17, Microsoft, 
   retrieved 7 May 2014

http://www.writinghighperf.net/go/21


1. Why Should You Choose Managed Code
----------------------------------------------------
    - Safety—The compiler and runtime can enforce type safety
	- Automatic memory management
    - Higher level of abstraction
	- Advanced language features—Delegates, anonymous methods, and dynamic typing
    - Huge existing code base—Framework Class Library, Entity Framework, Windows
	- Easier extensibility—With reflection capabilities, 
	  it is much easier to dynamically consume late-bound modules, such as in an extension architecture
	- Phenomenal debugging—Exceptions have a lot of information associated with them. 
	  All objects have metadata associated with them to allow thorough heap 
	  and stack analysis in a debugger, often without the need for PDBs (symbol files)
	  
2. Is Managed Code Slower Than Native Code.
----------------------------------------------------
    - That is, the first time a method is executed, the CLR will invoke the compiler on your IL 
	  to convert it to assembly code (e.g., x86, x64, ARM). 
	  Most code optimization happens at this stage. 
	  There is a definite performance hit on this first run, but after that you will always get the compiled version.
	  
	- Memory allocations
	  There is no contention for memory allocations on the heap, unlike in native applications. 
	  Some of the saved time is transferred to garbage collection, 
	  but even this can be mostly erased depending on how you configure your application.
	  
	- Fragmentation
	  Memory fragmentation that steadily gets worse over time is a common problem in large, 
	  long-running native applications. 
	  This is less of an issue in .NET applications because garbage collection will compact the heap.
	
	- JITted code–Because code is JITted as it is executed, 
	  its location in memory can be more optimal than that of native code. 
	  
3. Measurement Tools.
----------------------------------------------------

    - Visual Studio Professional performance wizard
	- Visual Studio Standalone Profiler
	- VsPerfCmd.exe
	- PerformanceMonitor (PerfMon.exe)
	- PerfView
	- CLR Profile
	- Windbg
	
	
4. Windows memory.
----------------------------------------------------

   Physical Memory
     The actual physical memory chips in a computer. 
	 Only the operating system manages physical memory directly.
	 
   Virtual Memory
     A logical organization of memory in a given process. 
	 Virtual memory size can be larger than physical memory. 
	 For example, 32-bit programs have a 4 GB address space, 
	 even if the computer itself only has 2 GB of RAM
	 Contiguous blocks of virtual memory may not be contiguous in physical memory. 
	 All memory addresses in a process are for the virtual memory.
	 
   Reserved Memory
     A region of virtual memory address space that has been reserved 
     for the process and thus will not be allocated to a future requester.
	 
   Committed Memory
     A region of memory that has a physical backing store. 
	 This can be RAM or disk.
	 
   Page
     An organizational unit of memory. 
	 Blocks of memory are allocated in a page, which is usually a few KB in size
	 
   Paging
     The process of transferring pages between regions of virtual memory. 
	 The page can move to or from another process (soft paging) or the disk (hard paging). 
	 Soft paging can be accomplished very quickly by mapping 
	 the existing memory into the current process’s virtual address space. 
	 Hard paging involves a relatively slow transfer of data to or from disk. 
	 Your program must avoid this at all costs to maintain good performance
	
   Context Switch
     The process of saving and restoring the state of a thread or process. 
     Because there are usually more running threads than available processors, 
	 there are often many context switches per second.
	 
   Kernel Mode
     A mode that allows the OS to modify low-level aspects of the hardware’s state, 
	 such as modifying certain registers or enabling/disabling interrupts. 
     Transitioning to Kernel Mode requires an operating system call, 
	 and can be quite expensive.
	 
   User Mode
     An unprivileged mode of executing instructions. 
	 There is no ability to modify low-level aspects of the system.

   User Mode
     An unprivileged mode of executing instructions. 
	 There is no ability to modify low-level aspects of the system. 
	 
 class Program
 {
    static List<string> times = new List<string>();
	
    static void Main(string[] args)
    {
        Console.WriteLine("Press any key to exit");
        while (!Console.KeyAvailable) 
        {
            times.Add(DateTime.Now.ToString());
            Console.Write('.');
            Thread.Sleep(1000);
        }
    }
 }

 5. Garbage Collection
 ----------------------------------------------------
 
 The native heap in Windows maintains free lists to know where to put new allocations. 
 Despite the use of low fragmentation heaps, many long-running native 
 code applications struggle with fragmentation. 
 
 Time spent in memory allocation gradually increases as the allocator spends more 
 and more time traversing the free lists looking for an open spot. Memory use continues to grow  
 and, inevitably, the process will need to be restarted to begin the cycle anew. Some native 
 programs deal with this by replacing the default implementation of malloc with custom 
 allocation schemes that work hard to reduce this fragmentation
 
 In .NET, memory allocation is trivial because it usually happens at the end of a memory segment 
 and is not much more than a few additions, decrements, and a comparison in the normal case. In 
 these simple cases, there are no free lists to traverse and little possibility of fragmentation. GC 
 heaps can actually be more efficient because objects allocated together in time tend to be near 
 one another on the heap, improving locality. 
 
  - Basic Operation
    The managed heap is further divided into two types of heaps: 
	the small object heap and the large object heap (LOH).
	
 There are three generations, referenced casually as gen 0, gen 1, and gen 2. 
 Gen 0 and gen 1 are always in the same segment, but gen 2 can span multiple segments, as can the large object heap
 
 The CLR allocates all objects that are less than 85,000 bytes in size on the small object heap. 
 They are always allocated in gen 0, usually at the end of the current used space
 
 When Gen0 is full, no new memory can be allocated and it may trigger GC
 Objects always begin their life in gen 0. As long as they are still alive, 
 the GC will promote them to subsequent generations each time a collection happens.
 
 When a garbage collection occurs, a compaction may occur, 
 in which case the GC physically moves the objects to a new location to free space in the segment.
 
 Compaction may occur in the collection of any generation and this is a relatively expensive 
 process because the GC must fix up all of the references to those objects so they point to the new 
 location, which may require pausing all managed threads. Because of this expense, the garbage 
 collector will only do compaction when it is productive to do so, based on some internal metrics

 GC roots: 
 
 A root can be the static variables in your program, the threads which have the stacks 
 (which will point to local variables) from all running methods, 
 strong GC handles (such as pinned handles), and the finalizer queue
 
 Note that you may have objects that no longer have roots to them, 
 but if the objects are in gen 2, then a gen 0 collection will not clean them up. 
 They will have to wait for a full collection
 
 There are four phases to a garbage collection:
  1. Suspension—All managed threads in the application 
     are forced to pause before a collection can occur. 
  2. Mark—Starting from each root, the garbage collector follows every object reference 
     and marks those objects as seen. 
  3. Compact—Reduce memory fragmentation by relocating objects to be next to each other 
     and update all references to point to the new locations. 
	 This happens on the small object heap when needed and there is no way to control it. 
	 On the large object heap, compaction does not happen automatically at all, 
	 but you can instruct the garbage collector to compact it on-demand.
  4. Resume—The managed threads are allowed to resume
  
 Another trigger for GCs is the total available memory on a machine, independent of your application. 
 If available memory drops below a certain threshold, garbage collection may happen more frequently 
 in an attempt to reduce the overall heap size

 Workstation vs. Server GC
 
   Workstation GC is the default. In this mode, all GCs happen on the same thread 
   that triggered the collection and run at the same priority
   
   Garbage collection happens in parallel. Each GC thread collects one of the heaps. 
   This can make garbage collection significantly faster than in workstation GC. 
   
 Background GC
   Background GC works by having a dedicated thread for garbage collecting generation 2
 
 Low Latency Mode
   LowLatency—For workstation GC only, it will suppress gen 2 collections.
   
 A good example is stock trading: 
 during market hours, you do not want full garbage collections happening. 
 When the market closes, you turn this mode off and perform full GCs until the market reopens
  
 The Most Important Rule
   Garbage collections get more expensive in each generation. 
   You want to ensure there are many gen 0/1 collections and very few gen 2 collections.  
   
 Reduce Object Lifetime
 Reduce Depth of Tree
 Reduce References between Objects
 
 Avoid Finalizers
 
 Never implement a finalizer unless it is required. 
 Finalizers are code, triggered by the garbage collector to cleanup unmanaged resources. 
 They are called from a single thread, one after the other, 
 and only after the garbage collector declares the object dead after a collection. 
 
 This means that if your class implements a finalizer, 
 you are guaranteeing that it will stay in memory even after the collection that should have killed it. 
 This decreases overall GC efficiency and ensures that your program 
 will dedicate CPU resources to cleaning up your object. 
 
 If you do implement a finalizer, you must also implement the IDisposable interface 
 to enable explicit cleanup, and call GC.SuppressFinalize(this) in the Dispose method 
 to remove the object from the finalization queue. 
 
 As long as you call Dispose before the next collection, 
 then it will clean up the object properly without the need for the finalizer to run.
  
class Foo : IDisposable
{
  ~Foo()
  {
    Dispose(false);
  }
  
  public void Dispose()
  {
    Dispose(true);
    GC.SuppressFinalize(this);
  }
  
  protected virtual void Dispose(bool disposing)
  {
    if (disposing)
    {
       this.managedResource.Dispose();
    }
	
    // Cleanup unmanaged resourced
    UnsafeClose(this.handle);
    // If the base class is IDisposable object 
    // make sure you call: 
    //base.Dispose(disposing);
  } 
}

Drawbacks of fimalizers: 

 - The finalizer is called when the GC detects that an object is eligible for collection. 
   This happens at some undetermined period of time after the resource is not needed anymore. 
   
 - It also provides the GC.SuppressFinalize method that can tell the GC 
   that an object was manually disposed of and does not need to be finalized anymore, 
   in which case the object’s memory can be reclaimed earlier.
   
 - Some people think that finalizers are guaranteed to run. 
   This is generally true, but not absolutely so. 
   If a program is force-terminated then no more code runs 
   and the process dies immediately. 
   
 - Moreover, because finalizers execute sequentially, 
   if another finalizer has an infinite loop bug in it, 
   then no finalizers after it will ever run. 
   
 Avoid Copying Buffers

 var memoryStream = new MemoryStream();
 var segment = new ArraySegment<byte>(memoryStream.GetBuffer(), 100, 1024);
 ...
 var blockStream = new MemoryStream(segment.Array, 
    segment.Offset, 
    segment.Count);
	
 The biggest problem with copying memory is not the CPU necessarily, but the GC. 
 If you find yourself needing to copy a buffer, then try to copy 
 it into another pooled or existing buffer to avoid any new memory allocations
 
 Pool Long-Lived and Large Objects
 
 Remember the cardinal rule from earlier: 
   Objects live very briefly or forever. 
   They either go away in gen 0 collections or last forever in gen 2.
   Another strong candidate for pooling is any object 
   that you allocate on the LOH heap, typically collections. 
   
 This was MemoryStream, which we used for serialization and transmitting bits over the network. 
 The actual implementation is more complex than just keeping a queue of MemoryStream objects 
 because of the need to avoid fragmentation, but conceptually, that is exactly what it is. 
 Every time a MemoryStream object was disposed, it was put back in the pool for reuse.
 
 Use Weak References for Caching
 
 Weak references are references to an object that still allow the garbage collector 
 to clean up the object.
 
 WeakReference weakRef = new WeakReference(myExpensiveObject);
 …
 // Create a strong reference to the object, 
 // now no longer eligible for GC
 var myObject = weakRef.Target;
 
 if (myObject != null)
 {
    myObject.DoSomethingAwesome();
 }
 
6. JIT
---------------------------------------------- 
Reducing JIT and Startup Time 

 - LINQ 
 - The dynamic keyword
 - Regular expressions
 - Code generation
 
7. Asynchronous Programming
  - Avoid Blocking
  - Use Tasks for Non-Blocking I/O
  - Adapt the Asynchronous Programming Model (APM) to Task

8.When to use tasks instead of async / await? 

Unfortunately, async/await will not handle this nondeterministic situation of arbitrarily 
choosing from multiple child Tasks that you are waiting on. 
One way to resolve this is to use the ContinueOnAny method described earlier. 
Alternatively, you could use TaskCompletionSource to manually control when the top level Task is complete.

----------------------------------------------
9. Do not create an excessive number of timers.

All Timers are serviced from a single thread in the thread pool. 
A huge number of Timers will cause delays in executing their callbacks.
Ensure Good Startup Thread Pool Size

Thread Synchronization and Locks
Thread synchronization is usually accomplished with synchronization objects 
such as Monitor, Semaphore, ManualResetEvent, and others.  
Locking never improves performance.

Inefficiant code:

object syncObj = new object();
var masterList = new List<long>();
const int NumTasks = 8;
Task[] tasks = new Task[NumTasks];

for (int i = 0; i < NumTasks; i++)
{ 
   tasks[i] = Task.Run(()=>
   {
        for (int j = 0; j < 5000000; j++)
        {
            lock (syncObj)
            {
                masterList.Add(j);
            }
        }
    });
}
Task.WaitAll(tasks);

Efficient code: 

object syncObj = new object();
var masterList = new List<long>();
const int NumTasks = 8;
Task[] tasks = new Task[NumTasks];

for (int i = 0; i < NumTasks; i++)
{ 
    tasks[i] = Task.Run(()=>
    {
        var localList = new List<long>();
        for (int j = 0; j < 5000000; j++)
        {
            localList.Add(j);
        }
		
        lock (syncObj)
        {
            masterList.AddRange(localList);
        }
    });
}
Task.WaitAll(tasks);

Synchronization Preference Order: 
1. No synchronization at all
2. Simple Interlocked methods
3. lock/Monitor class
4. Asynchronous locks (see later in this chapter)
5. Everything else

Memory Models
Use volatile When Necessary

private bool isComplete = false;
private object syncObj= new object();

// Incorrect implementation!
private void Complete()
{
    if (!isComplete)
    {
        lock (syncObj)
        {
            if (!isComplete)
            {
                DoCompletionWork();
                isComplete = true;
            }
        }
    }
}

leading to multiple threads seeing a false value even after another thread has set it to true. 

Use Interlocked Method

private bool isComplete = false;
private object completeLock = new object();

private void Complete()
{
   lock(completeLock)
   {
      if (isComplete)
      {
         return;
      }
      isComplete = true;
   }
   ...
}

private int isComplete = 0;
private void Complete()
{
   if (Interlocked.Increment(ref isComplete) == 1)
   {
      ...
   }
}

CompareAndExchange—
  Accepts values A, B, and C. 
  Compares values A and C, and, if equal, replaces A with B, and returns original value. 
  
  if (A == C) 
  {
     var original = A;
     A = B;
	 return original;
  }  

class LockFreeStack<T>
{
   private class Node
   {  
      public T Value;
      public Node Next;
   }
   
   private Node head;
   
   public void Push(T value)
   {
      var newNode = new Node() { Value = value }; 
	  
	  while (true)
      {
         newNode.Next = this.head;
		 
         if (Interlocked.CompareExchange(ref this.head, newNode, newNode.Next) == newNode.Next)
         {
            return;
         }
      }
   }

// if (this.head == newNode.Next)
   {
      var prev = this.head; 
      this.head = newnode;
	  return prev;
   }
   
   public T Pop()
   {
      while (true)
      {
         Node node = this.head;
         if (node == null)
         {
            return default(T);
         }
         if (Interlocked.CompareExchange(ref this.head, node.Next, node) == node)
         {
            return node.Value;
         }
      }
   } 
}
   
10. Asynchronous Lock

The SemaphoreSlim class has a WaitAsync method that returns a Task. 
Instead of blocking on a wait, you can schedule a continuation on the Task 
that will execute once the semaphore permits it to run.

const int Size = 256;
static int[] array = new int[Size];
static int length = 0;
static SemaphoreSlim semaphore = new SemaphoreSlim(1);

static void Main(string[] args)
{
   var writerTask = Task.Run((Action)WriterFunc);
   var readerTask = Task.Run((Action)ReaderFunc);
   Console.WriteLine("Press any key to exit");
   Console.ReadKey();
}

static void WriterFunc()
{
   while (true)
   {
      semaphore.Wait();
        Console.WriteLine("Writer: Obtain");
        for (int i = length; i < array.Length; i++)
        {
           array[i] = i * 2;
        }
        Console.WriteLine("Writer: Release");
      semaphore.Release();
   }
}

static void ReaderFunc()
{
   while (true)
   {
      semaphore.Wait();
        Console.WriteLine("Reader: Obtain");
        for (int i = length; i >= 0; i--)
        {
           array[i] = 0;
        }
        length = 0;
        Console.WriteLine("Reader: Release");
      semaphore.Release();
    }
}

----------------------------
async locks: 

static void WriterFuncAsync()
{ 
    semaphore.WaitAsync().ContinueWith(_ =>
    {
       Console.WriteLine("Writer: Obtain");
       for (int i = length; i < array.Length; i++)
       {
          array[i] = i * 2;
       }
       Console.WriteLine("Writer: Release");
       semaphore.Release();
    })
	.ContinueWith(_=>WriterFuncAsync()); 
}

static void ReaderFuncAsync()
{ 
    semaphore.WaitAsync().ContinueWith(_ =>
    {
       Console.WriteLine("Reader: Obtain");
       for (int i = length; i >= 0; i--)
       {
          array[i] = 0;
       }
       length = 0;
       Console.WriteLine("Reader: Release");
       semaphore.Release();
    }).ContinueWith(_=>ReaderFuncAsync()); 
}

In many cases, making batch updates under a single lock is better 
than taking a lock for every single small update. 
In my own informal testing, I found that inserting 
an item into a ConcurrentDictionary is about 2x slower than with a standard Dictionary

1. Replace an Entire Collection

If your data is mostly read-only then you can safely use a non-concurrent collection 
when accessing it. When it is time to update the collection, 
you can generate a new collection object entirely and just replace the original.

private volatile Dictionary<string, MyComplexObject> data = new Dictionary<string, MyComplexObject>();

public Dictionary<string, MyComplexObject> Data { get { return data; } }

private void UpdateData()
{
    var newData = new Dictionary<string, MyComplexObject>();
    newData["Foo"] = new MyComplexObject();
    ...
    data = newData;
}

2. Copy Your Resource Per-Thread

----------------------------------------------------------------
General Coding and Class Design
Why this code will not compile: 

struct Point
{
   public int x;
   public int y;
}

public static void Main()
{
   List<Point> points = new List<Point>();
   points.Add(new Point() { x = 1, y = 2 });
   points[0].x = 3;
}

points[0] returns a copy of the original value,

 Point p = points[0];
 p.x = 3;
 points[0] = p;

class Order
{
  public OrderTimes Times;
}
OrderTimes is struct 

-------------------------------------------------
Override Equals and GetHashCode for Struct

If you just override Equals(object other) - 
then method will involve casting and boxing on value types.
Instead, implement Equals(T other), where T is the type of your struct.
During compilation, the compiler 
will prefer the more strongly typed version whenever possible.

struct Vector : IEquatable<Vector>
{
   public int X { get; set; }
   public int Y { get; set; }
   public int Z { get; set; }
   public int Magnitude { get; set; }
   
   public override bool Equals(object obj)
   {
     if (obj == null)
     {
        return false;
     }
	 
     if (obj.GetType() != this.GetType())
     {
        return false;
     }
	 
     return this.Equals((Vector)obj);
   }
   
   public bool Equals(Vector other)
   {
     return this.X == other.X
         && this.Y == other.Y
         && this.Z == other.Z
         && this.Magnitude == other.Magnitude;
   }
   
   public override int GetHashCode()
   {
     return X ^ Y ^ Z ^ Magnitude;
   }
}

Virtual Methods and Sealed Classe
 Making methods virtual prevents certain optimizations by the JIT compiler, 
 notably the ability to inline them.
 
Interface Dispatch
 Pick your common base interface and replace it with an abstract base class instead
 The performance of an abstract class is fast. 
 The performance of interface is slow because it requires time 
 to search actual method in the corresponding class 
 
--------------------------------------------------------
Avoid Boxing

int x = 32;
object o = x

Avoid String.Format 
Boxing can also occur when assigning a struct to an interface

interface INameable
{
   string Name { get; set; }
}

struct Foo : INameable
{
   public string Name { get; set; } 
}

void TestBoxing()
{ 
   Foo foo = new Foo() { Name = "Bar" };
   // This boxes!
   INameable nameable = foo;
   ...
}

int val = 13;
object boxedVal = val;
val = 14;

Finally, note that passing a value type by reference is not boxing. 
Examine the IL and you will see that no boxing occurs. 
The address of the value type is sent to the method.

-----------------------------------
for vs. foreach
Casting
Casting to an interface is more expensive than casting to a concrete type
Never have this pattern, which performs two casts:

if (a is Foo)
{
   Foo f = (Foo)a;
}

Foo f = a as Foo;
if (f != null)
{
   ...
}

---------------------------------
Delegates

There are two costs associated with use of delegates: construction and invocation. 
Invocation, thankfully, is comparable to a normal method call in nearly all circumstances, 
but delegates are objects and constructing them can be quite expensive.

-------------------------------
Exceptions

In .NET, putting a try block around code is cheap, 
but exceptions are very expensive to throw. 
This is largely because of the rich state that .NET exceptions contain.

-----------------------------------------------------------------------
Dynamic
Code Generation

Not only does Activator.CreateInstance use significant CPU, 
but it can cause unnecessary allocations, 
which put extra pressure on the garbage collector.

-----------------------------------------------------------
Collections 

SortedDictionary is implemented as a binary search tree and has O(log n) insertion and retrieval times.
SortedList is implemented as a sorted array. 
It has O(log n) retrieval times, but can have O(n) insertion times in the worst case.

List, Stack, and Queue all use arrays internally 
and thus have good locality of reference for efficient operations on many values, 
however when adding a lot of values, 
they will resize these internal arrays as needed.

List has O(1) insertion, but O(n) removal and searching.
LinkedList has O(1) insertion and removal characteristics, 
but it should be avoided for primitive types because it will allocate a new LinkedListNode object 
for every item you add, which can be wasteful overhead.

----------------------------------------------
Strings 

Avoid calling methods like ToLower and ToUpper, 
especially if you are doing this for string comparison purposes. 
Instead, use one of the IgnoreCase options for the String.Compare method.

string result = a + b + c + d + e + f;

Do not consider StringBuilder until the number of strings 
is variable and likely larger than a few dozen.






























 











   












  





  


   


 
  

 

 




  


  
    
   

 
 
  
  







 





 
 
 
	




	


 
   



	  




	