
1. Hewardt, Mario and Patrick Dussud. Advanced .NET Debugging. 
   Addison-Wesley  Professional, November 2009.
2  Richter, Jeffrey. CLR via C#, 4th ed. Microsoft Press, November 2012.
3  Russinovich, Mark and David Solomon, Alex Ionescu. Windows Internals, 6th ed. 
   Microsoft Press, March 2012.
4  Rasmussen, Brian. High-Performance Windows Store Apps. Microsoft Press, May 2014.
5  ECMA C# and CLI Standards: http://www.writinghighperf.net/go/17, Microsoft, 
   retrieved 7 May 2014



1. Why Should You Choose Managed Code
----------------------------------------------------
    - Safety—The compiler and runtime can enforce type safety
	- Automatic memory management
    - Higher level of abstraction
	- Advanced language features—Delegates, anonymous methods, and dynamic typing
    - Huge existing code base—Framework Class Library, Entity Framework, Windows
	- Easier extensibility—With reflection capabilities, 
	  it is much easier to dynamically consume late-bound modules, such as in an extension architecture
	- Phenomenal debugging—Exceptions have a lot of information associated with them. 
	  All objects have metadata associated with them to allow thorough heap 
	  and stack analysis in a debugger, often without the need for PDBs (symbol files)
	  
2. Is Managed Code Slower Than Native Code.
----------------------------------------------------
    - That is, the first time a method is executed, the CLR will invoke the compiler on your IL 
	  to convert it to assembly code (e.g., x86, x64, ARM). 
	  Most code optimization happens at this stage. 
	  There is a definite performance hit on this first run, but after that you will always get the compiled version.
	  
	- Memory allocations
	  There is no contention for memory allocations on the heap, unlike in native applications. 
	  Some of the saved time is transferred to garbage collection, 
	  but even this can be mostly erased depending on how you configure your application.
	  
	- Fragmentation
	  Memory fragmentation that steadily gets worse over time is a common problem in large, 
	  long-running native applications. 
	  This is less of an issue in .NET applications because garbage collection will compact the heap.
	
	- JITted code–Because code is JITted as it is executed, 
	  its location in memory can be more optimal than that of native code. 
	  
3. Measurement Tools.
----------------------------------------------------

    - Visual Studio Professional performance wizard
	- Visual Studio Standalone Profiler
	- VsPerfCmd.exe
	- PerformanceMonitor (PerfMon.exe)
	- PerfView
	- CLR Profile
	- Windbg
	
	
4. Windows memory.
----------------------------------------------------

   Physical Memory
     The actual physical memory chips in a computer. 
	 Only the operating system manages physical memory directly.
	 
   Virtual Memory
     A logical organization of memory in a given process. 
	 Virtual memory size can be larger than physical memory. 
	 For example, 32-bit programs have a 4 GB address space, 
	 even if the computer itself only has 2 GB of RAM
	 Contiguous blocks of virtual memory may not be contiguous in physical memory. 
	 All memory addresses in a process are for the virtual memory.
	 
   Reserved Memory
     A region of virtual memory address space that has been reserved 
     for the process and thus will not be allocated to a future requester.
	 
   Committed Memory
     A region of memory that has a physical backing store. 
	 This can be RAM or disk.
	 
   Page
     An organizational unit of memory. 
	 Blocks of memory are allocated in a page, which is usually a few KB in size
	 
   Paging
     The process of transferring pages between regions of virtual memory. 
	 The page can move to or from another process (soft paging) or the disk (hard paging). 
	 Soft paging can be accomplished very quickly by mapping 
	 the existing memory into the current process’s virtual address space. 
	 Hard paging involves a relatively slow transfer of data to or from disk. 
	 Your program must avoid this at all costs to maintain good performance
	
   Context Switch
     The process of saving and restoring the state of a thread or process. 
     Because there are usually more running threads than available processors, 
	 there are often many context switches per second.
	 
   Kernel Mode
     A mode that allows the OS to modify low-level aspects of the hardware’s state, 
	 such as modifying certain registers or enabling/disabling interrupts. 
     Transitioning to Kernel Mode requires an operating system call, 
	 and can be quite expensive.
	 
   User Mode
     An unprivileged mode of executing instructions. 
	 There is no ability to modify low-level aspects of the system.

   User Mode
     An unprivileged mode of executing instructions. 
	 There is no ability to modify low-level aspects of the system. 
	 
 class Program
 {
    static List<string> times = new List<string>();
	
    static void Main(string[] args)
    {
        Console.WriteLine("Press any key to exit");
        while (!Console.KeyAvailable) 
        {
            times.Add(DateTime.Now.ToString());
            Console.Write('.');
            Thread.Sleep(1000);
        }
    }
 }

 5. Garbage Collection
 ----------------------------------------------------
 
 The native heap in Windows maintains free lists to know where to put new allocations. 
 Despite the use of low fragmentation heaps, many long-running native 
 code applications struggle with fragmentation. 
 
 Time spent in memory allocation gradually increases as the allocator spends more 
 and more time traversing the free lists looking for an open spot. Memory use continues to grow  
 and, inevitably, the process will need to be restarted to begin the cycle anew. Some native 
 programs deal with this by replacing the default implementation of malloc with custom 
 allocation schemes that work hard to reduce this fragmentation
 
 In .NET, memory allocation is trivial because it usually happens at the end of a memory segment 
 and is not much more than a few additions, decrements, and a comparison in the normal case. In 
 these simple cases, there are no free lists to traverse and little possibility of fragmentation. GC 
 heaps can actually be more efficient because objects allocated together in time tend to be near 
 one another on the heap, improving locality. 
 
  - Basic Operation
    The managed heap is further divided into two types of heaps: 
	the small object heap and the large object heap (LOH).
	
 There are three generations, referenced casually as gen 0, gen 1, and gen 2. 
 Gen 0 and gen 1 are always in the same segment, but gen 2 can span multiple segments, as can the large object heap
 
 The CLR allocates all objects that are less than 85,000 bytes in size on the small object heap. 
 They are always allocated in gen 0, usually at the end of the current used space
 
 When Gen0 is full, no new memory can be allocated and it may trigger GC
 Objects always begin their life in gen 0. As long as they are still alive, 
 the GC will promote them to subsequent generations each time a collection happens.
 
 When a garbage collection occurs, a compaction may occur, 
 in which case the GC physically moves the objects to a new location to free space in the segment.
 
 Compaction may occur in the collection of any generation and this is a relatively expensive 
 process because the GC must fix up all of the references to those objects so they point to the new 
 location, which may require pausing all managed threads. Because of this expense, the garbage 
 collector will only do compaction when it is productive to do so, based on some internal metrics

 GC roots: 
 
 A root can be the static variables in your program, the threads which have the stacks 
 (which will point to local variables) from all running methods, 
 strong GC handles (such as pinned handles), and the finalizer queue
 
 Note that you may have objects that no longer have roots to them, 
 but if the objects are in gen 2, then a gen 0 collection will not clean them up. 
 They will have to wait for a full collection
 
 There are four phases to a garbage collection:
  1. Suspension—All managed threads in the application 
     are forced to pause before a collection can occur. 
  2. Mark—Starting from each root, the garbage collector follows every object reference 
     and marks those objects as seen. 
  3. Compact—Reduce memory fragmentation by relocating objects to be next to each other 
     and update all references to point to the new locations. 
	 This happens on the small object heap when needed and there is no way to control it. 
	 On the large object heap, compaction does not happen automatically at all, 
	 but you can instruct the garbage collector to compact it on-demand.
  4. Resume—The managed threads are allowed to resume
  
 Another trigger for GCs is the total available memory on a machine, independent of your application. 
 If available memory drops below a certain threshold, garbage collection may happen more frequently 
 in an attempt to reduce the overall heap size

 Workstation vs. Server GC
 
   Workstation GC is the default. In this mode, all GCs happen on the same thread 
   that triggered the collection and run at the same priority
   
   Garbage collection happens in parallel. Each GC thread collects one of the heaps. 
   This can make garbage collection significantly faster than in workstation GC. 
   
 Background GC
   Background GC works by having a dedicated thread for garbage collecting generation 2
 
 Low Latency Mode
   LowLatency—For workstation GC only, it will suppress gen 2 collections.
   
 A good example is stock trading: 
 during market hours, you do not want full garbage collections happening. 
 When the market closes, you turn this mode off and perform full GCs until the market reopens
  
 The Most Important Rule
   Garbage collections get more expensive in each generation. 
   You want to ensure there are many gen 0/1 collections and very few gen 2 collections.  
   
 Reduce Object Lifetime
 Reduce Depth of Tree
 Reduce References between Objects
 
 Avoid Finalizers
 
 Never implement a finalizer unless it is required. 
 Finalizers are code, triggered by the garbage collector to cleanup unmanaged resources. 
 They are called from a single thread, one after the other, 
 and only after the garbage collector declares the object dead after a collection. 
 
 This means that if your class implements a finalizer, 
 you are guaranteeing that it will stay in memory even after the collection that should have killed it. 
 This decreases overall GC efficiency and ensures that your program 
 will dedicate CPU resources to cleaning up your object. 
 
 If you do implement a finalizer, you must also implement the IDisposable interface 
 to enable explicit cleanup, and call GC.SuppressFinalize(this) in the Dispose method 
 to remove the object from the finalization queue. 
 
 As long as you call Dispose before the next collection, 
 then it will clean up the object properly without the need for the finalizer to run.
  
class Foo : IDisposable
{
  ~Foo()
  {
    Dispose(false);
  }
  
  public void Dispose()
  {
    Dispose(true);
    GC.SuppressFinalize(this);
  }
  
  protected virtual void Dispose(bool disposing)
  {
    if (disposing)
    {
       this.managedResource.Dispose();
    }
	
    // Cleanup unmanaged resourced
    UnsafeClose(this.handle);
    // If the base class is IDisposable object 
    // make sure you call: 
    //base.Dispose(disposing);
  } 
}

Drawbacks of fimalizers: 

 - The finalizer is called when the GC detects that an object is eligible for collection. 
   This happens at some undetermined period of time after the resource is not needed anymore. 
   
 - It also provides the GC.SuppressFinalize method that can tell the GC 
   that an object was manually disposed of and does not need to be finalized anymore, 
   in which case the object’s memory can be reclaimed earlier.
   
 - Some people think that finalizers are guaranteed to run. 
   This is generally true, but not absolutely so. 
   If a program is force-terminated then no more code runs 
   and the process dies immediately. 
   
 - Moreover, because finalizers execute sequentially, 
   if another finalizer has an infinite loop bug in it, 
   then no finalizers after it will ever run. 
   
 Avoid Copying Buffers

 var memoryStream = new MemoryStream();
 var segment = new ArraySegment<byte>(memoryStream.GetBuffer(), 100, 1024);
 ...
 var blockStream = new MemoryStream(segment.Array, 
    segment.Offset, 
    segment.Count);
	
 The biggest problem with copying memory is not the CPU necessarily, but the GC. 
 If you find yourself needing to copy a buffer, then try to copy 
 it into another pooled or existing buffer to avoid any new memory allocations
 
 Pool Long-Lived and Large Objects
 
 Remember the cardinal rule from earlier: 
   Objects live very briefly or forever. 
   They either go away in gen 0 collections or last forever in gen 2.
   Another strong candidate for pooling is any object 
   that you allocate on the LOH heap, typically collections. 
   
 This was MemoryStream, which we used for serialization and transmitting bits over the network. 
 The actual implementation is more complex than just keeping a queue of MemoryStream objects 
 because of the need to avoid fragmentation, but conceptually, that is exactly what it is. 
 Every time a MemoryStream object was disposed, it was put back in the pool for reuse.
 
 Use Weak References for Caching
 
 Weak references are references to an object that still allow the garbage collector 
 to clean up the object.
 
 WeakReference weakRef = new WeakReference(myExpensiveObject);
 …
 // Create a strong reference to the object, 
 // now no longer eligible for GC
 var myObject = weakRef.Target;
 
 if (myObject != null)
 {
    myObject.DoSomethingAwesome();
 }
 
6. JIT
---------------------------------------------- 
Reducing JIT and Startup Time 

 - LINQ 
 - The dynamic keyword
 - Regular expressions
 - Code generation
 
7. Asynchronous Programming
  - Avoid Blocking
  - Use Tasks for Non-Blocking I/O
  - Adapt the Asynchronous Programming Model (APM) to Task

8.When to use tasks instead of async / await? 

Unfortunately, async/await will not handle this nondeterministic situation of arbitrarily 
choosing from multiple child Tasks that you are waiting on. 
One way to resolve this is to use the ContinueOnAny method described earlier. 
Alternatively, you could use TaskCompletionSource to manually control when the top level Task is complete.


  





  


   


 
  

 

 




  


  
    
   

 
 
  
  







 





 
 
 
	




	


 
   



	  




	